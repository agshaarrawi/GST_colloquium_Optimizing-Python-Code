{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavis1/GST_colloquium/blob/main/GST_colloquium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GST Colloquium on Python coding\n",
        "\n",
        "\n",
        "*   random grab-bag of tips and tricks\n",
        "*   hyper-optimization is not encouraged in Python but these techniques can provide large performace gains for little-to-no extra work\n",
        "*   In many cases these optimizations don't matter much and we've chosen small examples for practicality but when working on large datasets they can provide major practical improvements\n",
        "\n"
      ],
      "metadata": {
        "id": "dvSqLtCbA3vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING ONLY\n",
        "#this allows us to interact with google drive from within python\n",
        "from google.colab import drive\n",
        "#we mount our google drive which means that we make it acessable as a hard drive\n",
        "#in our file system under the path '/content/drive'\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/GST_colloquium/* ."
      ],
      "metadata": {
        "id": "sC98RVF-BDsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#colab comes with most common packages installed already but we want to use a\n",
        "#few uncommon ones so we install these ourselves.\n",
        "#The \"!\" tells jupyter to pass the line to the bash shell, this allows us to run\n",
        "#command line tools like pip from within jupyter notebook.\n",
        "!pip install brain-isotopic-distribution\n",
        "!pip install multiprocesspandas\n",
        "!pip install xlsx2csv"
      ],
      "metadata": {
        "id": "gTQu0A6eB3qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it is generally recommended best practice to import the packages that come with\n",
        "#python first before importing third party packages.\n",
        "from collections import defaultdict\n",
        "from functools import cache\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "from io import StringIO\n",
        "\n",
        "from brainpy import isotopic_variants\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocesspandas\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import zscore\n",
        "from sortedcontainers import SortedList\n",
        "from xlsx2csv import Xlsx2csv"
      ],
      "metadata": {
        "id": "_M3C91KvBy-K"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This defines a function decorator, a concept which is used in ยง1.2 and ยง4.1\n",
        "#It times a function call and prints the elapsed time while suppressing warnings\n",
        "def time_this(func):\n",
        "  #the way this works is that time_this() creates and returns a new function\n",
        "  #that replaces whatever funciton follows @time_this. But, because\n",
        "  def inner(*args, **kwargs):\n",
        "    #this context manager suppresses all warnings. DO NOT DO THIS NORMALLY\n",
        "    #We have extensively tested this code and data and know that the warnings\n",
        "    #can be safely ignored for this tutorial but that is not true for most code\n",
        "    with warnings.filterwarnings('ignore'):\n",
        "      start_time = time.process_time()\n",
        "      result = func(*args, **kwargs)\n",
        "      elapsed_time = time.process_time() - start_time\n",
        "    print(f'Running {func.__name__} took {elapsed_time} seconds.')\n",
        "    return result\n",
        "  return inner\n",
        "\n",
        "#A test version without the warning suppression\n",
        "def time_this(func):\n",
        "  def inner(*args, **kwargs):\n",
        "    start_time = time.process_time()\n",
        "    result = func(*args, **kwargs)\n",
        "    elapsed_time = time.process_time() - start_time\n",
        "    print(f'Running {func.__name__} took {elapsed_time} seconds.')\n",
        "    return result\n",
        "  return inner"
      ],
      "metadata": {
        "id": "1SOGhargJ13-"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1: Data Structures\n",
        "\n",
        "The way data are stored in memory can have an enormous effect on the speed with which they can be processed for particular tasks. Choosing the correct data structure can be the difference between a runtime of hours or minutes in some situations."
      ],
      "metadata": {
        "id": "JJvq3pfCHxEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 Hashmaps\n",
        "\n",
        "Hashmaps are data structures that store key value pairs in a way that allows you to very quickly look up a value based on a key. Dictionaries in python are hashmaps which can store any object as a value and can use any hashable object as a key. Sets are specialized hashmaps which only store true or false as values. These can be used to efficiently look up whether or not a key is in the set.\n",
        "\n",
        "A hash is a function that maps objects of arbitrary size and complexity to a finite range of numbers. In the context of a hashmap data structure the finite range represents locations in memory and the hash function calculates where in memory a value is stored based on its key which means that the time it takes to find a value is only dependent on the time it takes to calculate the hash of a key. This contrasts with looking up values in a list which requires iterating over each item in the list until you find what you're looking for, an operation that becomes more expensive on average as the list grows in size.\n",
        "\n",
        "\n",
        "<img align=\"left\" src=\"https://github.com/m-j-keller/GST_colloquium/blob/main/Hashmapfigure-margin.png?raw=true\" alt=\"Figure 1. Hashmap Data Call\" title=\"Figure 1: Accessing Data in a Hashmap\" width=\"440\" hspace=\"20\" vspace=\"10\"> The tradeoff that you have to make with hashmaps is that there are some cases in which two keys have the same hash so their values must be stored in a list and iterated over for lookup. To minimize this problem the range of memory locations needs to be significantly larger than the number of values stored. This means that hash maps will generally be larger in memory than more compact data structures like lists. This is not as much of a problem as it first appears because  Python does not store the actual value at the memory locaiton calculated by the hash function. Instead the memory location it calculates is the index of a list that stores memory addresses which point to where the values are actually stored. This means that each block of memory indexed by the hashmap can be very small while still being able to point to values of arbitrary size. Therefore the memory overhead is only likely to be siginficant if you're storing a large number of small things.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**TLDR:** Storing data in hashmaps (like a python dictionary) can significantly improve data access speed, thereby speeding up the whole script.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DBtrYU4mIrTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 1: file parsing"
      ],
      "metadata": {
        "id": "wTzgj_HwJOyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2: Memoization\n"
      ],
      "metadata": {
        "id": "eWRNHC19hRsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 2: isotope abundance calculations"
      ],
      "metadata": {
        "id": "8xdqdzaoJan6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3: Search trees"
      ],
      "metadata": {
        "id": "1x8476GZIz9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 3: fuzzy matching"
      ],
      "metadata": {
        "id": "_yuGS-suI6Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2: Pandas and Numpy"
      ],
      "metadata": {
        "id": "fmc_HwHBHzip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1: Faster .xlsx import\n",
        "Large .xlsx files can be slow to import with pandas. This is because they are actually zipped folders of mostly XML files which need to be decompressed before going through the somewhat inefficient process of XML parsing. This setup gives excel the flexability to store images, graphs, formatting, and functions but, if all you want is a table of data, this flexability comes at significant performance costs. Thus it is generally preferable to store tabular data as a text file (.csv or .tsv) unless you absolutely need to store these   extra types of information.\n",
        "\n",
        "If you have no choice but to use large .xlsx files there is still a faster way to import the data as a pandas dataframe. The xlsx2csv package is sigificantly faster at parsing .xlsx files than pandas. So, what we can do is parse the .xlsx file to an in-memory .csv (meaning that the file is stored only in RAM and not written to our hard drive) then use the very efficient pandas .csv parser to get the data into a dataframe. This trick comes from [stackoverflow](https://stackoverflow.com/questions/28766133/faster-way-to-read-excel-files-to-pandas-dataframe#comment136215400_28769537).\n"
      ],
      "metadata": {
        "id": "C6KC1t-RN9To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 4: metabolomics data\n",
        "metabolomics_data.xlsx is an example of the files produced during metabolomics experiments in our lab. It is an export of unfiltered search results from Compound Discoverer which we use for downstream analysis.\n",
        "\n",
        "For practicality purposes we have reduced the size of the .xlsx file for this demonstration significantly. In its original form (256547 rows by 506 columns) pandas .read_excel() took 16 minutes to load the file, the Xlsx2csv trick took 7 minutes, and reading in the .tsv version of the file with pandas .read_csv() took 16 seconds. The remarkable increase in speed between the original document and its current version is not merely due to cutting the number of rows in half. The extra formatting information that was included in the export, which was stripped during our file processing that went .xlsx -> .tsv -> .xlsx,  has a far larger impact. This can be seen in the relatively modest improvement in load time for the tsv reader as compared to the impressive improvements for the two direct .xlsx readers. We expect that these improvements are due to the removed formatting having massively complicated the XML representation of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "DJVuSrn9Dq1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The simple .tsv import\n",
        "@time_this\n",
        "def tsv_import(file_path):\n",
        "  df = pd.read_csv(file_path, sep = '\\t')\n",
        "  return df\n",
        "\n",
        "#The in-memory .csv conversion trick described above\n",
        "@time_this\n",
        "def fast_import(file_path):\n",
        "    #buffer is the in-memory variable in which the converted .csv is placed\n",
        "    buffer = StringIO()\n",
        "    #here we do the actual .xlsx parsing and conversion to .csv\n",
        "    #note that skip_hidden_rows is true by default unlike other .xlsx readers\n",
        "    Xlsx2csv(file_path,\n",
        "             outputencoding=\"utf-8\",\n",
        "             skip_hidden_rows = False).convert(buffer,sheetid=0)\n",
        "    buffer.seek(0)\n",
        "    #now we import the .csv with pandas exactly as if it were a regular file\n",
        "    df = pd.read_csv(buffer, low_memory=False, skiprows=1)\n",
        "    return df\n",
        "\n",
        "#the pandas native approach to excel import\n",
        "@time_this\n",
        "def pandas_import(file_path):\n",
        "  df = pd.read_excel(file_path, 0)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Lz-JyysMDvIV"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_df = tsv_import('metabolomics_data.tsv')\n",
        "fast_df = fast_import('metabolomics_data.xlsx')\n",
        "pandas_df = pandas_import('metabolomics_data.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIcKCg3PFDND",
        "outputId": "8fd15207-a29a-497a-c16e-72ec24a649b5"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tsv_import took 11.185811518999799 seconds.\n",
            "Running fast_import took 60.39173324799958 seconds.\n",
            "Running pandas_import took 104.13923784999997 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2: Using vectorized operations\n",
        "Numpy carries out most data processing operations using highly optimized pre-compiled code that is orders of magnitude faster than anything Python can natively accomplish. The optimizations include single instruction, multiple data (SIMD) steps in which a single operation that would normally be applied serially to values in an array can be applied simultaniously to blocks of 4-8 values. When doing mathematical operations it is therefore best to let numpy handle as much of the processing as possible. This means that we should avoid writing explicit loops or accessing individual values within numpy arrays as much as possible and instead perform operations on whole arrays at once using numpy functions.\n",
        "\n",
        "This strategy holds true for other packages like Pandas, Scipy, and Scikit-Learn as well because Numpy provides most of the data processing functionality for these packages.  "
      ],
      "metadata": {
        "id": "yjU_X23DDUpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 5: normalization\n",
        "Proteomic normalization and imputation are important to minimize non-biological variation and ensure reliability of your dataset. Normalization is used to minimize variation coming from factors such as sample preparation inconsistencies, injection volume errors, and fluctuations in instrument sensitivity, which can happen with extended use (i.e. sensitivity drift with instrument cleanliness, or column age). By accounting for these variations, normalization allows for the comparison of biological variances rather than technical ones. Following normalization, imputation fills missing values with low abundance numbers that mimic the instrument's noise floor, allowing for statistical testing across conditions. Both normalization and imputation create a baseline from which conditions can be compared in a more robust way.\n"
      ],
      "metadata": {
        "id": "PCnOKINlDvm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The data processing produces one file per condition that we read into a list\n",
        "#of dataframes\n",
        "prot_files = [f for f in os.listdir() if f.endswith('Proteins.txt')]\n",
        "dataframes = [pd.read_csv(f, sep = '\\t') for f in prot_files]\n",
        "key_cols = ['Accession','Description','MW [kDa]','# AAs']\n",
        "#We need to merge these data into a single dataframe for further processing\n",
        "proteins = dataframes.pop()\n",
        "for df in dataframes:\n",
        "  #on = key_cols is the list of columns that we use as a key for which rows to merge\n",
        "  #how = 'outer' means we use the union of these keys\n",
        "  proteins = proteins.merge(df, how = 'outer', on = key_cols)\n",
        "#we want a list of the columns containing protein abundances for processing\n",
        "abundance_cols = [c for c in proteins.columns if c.startswith('Abundance')]\n",
        "#For this example we only want to work with the key and abundance columns\n",
        "#so we get rid of all the other data in this dataframe.\n",
        "#The * operator unpacks an interable so [1,*[2,3,4]] becomes [1,2,3,4].\n",
        "proteins = proteins[[*key_cols, *abundance_cols]]"
      ],
      "metadata": {
        "id": "QGP0UvJnUORv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(1)\n",
        "\n",
        "@time_this\n",
        "def vector_norm_impute(values):\n",
        "  #processing is done in the log space because intensity is log-normally\n",
        "  #distributed across proteins\n",
        "  values = np.log(values)\n",
        "  #we calculated the mean and standard deviation of all values in all samples\n",
        "  gmean = np.nanmean(values)\n",
        "  gstd = np.nanstd(values)\n",
        "  #intensities are z-scored by column (axis = 0), which corresponds to files\n",
        "  values = zscore(values, axis = 0, nan_policy = 'omit')\n",
        "  #we un-do the z-scoring with the grand mean and standard deviation so that\n",
        "  #all files now have the same log-mean and log-standard deviation\n",
        "  values = (values*gstd)+gmean\n",
        "  #we impute by taking draws from a normal distribution centered 1.8 standard\n",
        "  #deviations below the mean to simulate the correlation between missingness and\n",
        "  #intensity\n",
        "  loc = gmean - (1.8*gstd)\n",
        "  scale = gstd*0.3\n",
        "  #The strategy here is to generate a random number for each value in our data\n",
        "  #and then only transfer random values to our data at the missing value\n",
        "  #positions. We do generate significantly more random values than necessary\n",
        "  #this way but the vectorized approach is so much faster than repeated calls to\n",
        "  #rng that it is still the preferred approach\n",
        "  imputed_values = rng.normal(loc, scale, values.shape)\n",
        "  mask = np.isnan(values) #an array of true/false for if a value is missing\n",
        "  values[mask] = imputed_values[mask] #only missing values are replaced\n",
        "  #the data are taken back out of log space for downstream processing\n",
        "  values = np.exp(values)\n",
        "  return values\n",
        "\n",
        "@time_this\n",
        "def loop_norm_impute(values):\n",
        "  #this is how you do a nested loop within a list comprehension\n",
        "  logvalues = [np.log(value) for sample in values for value in sample]\n",
        "  #as above we pre-calculate variables that depend on all values\n",
        "  gmean = np.nanmean(logvalues)\n",
        "  gstd = np.nanstd(logvalues)\n",
        "  loc = gmean - (1.8*gstd)\n",
        "  scale = gstd*0.3\n",
        "  #in this verison we construct a list of lists\n",
        "  new_values = []\n",
        "  for i in range(values.shape[1]):\n",
        "    sample = values[:,i]\n",
        "    new_sample = []\n",
        "    #in order to do the normalization elementwise we have to precalculate\n",
        "    #sample-specific values\n",
        "    smean = np.nanmean(sample)\n",
        "    sstd = np.nanstd(sample)\n",
        "    for value in sample:\n",
        "      #all the processing from here out is the same but done on one element\n",
        "      #at a time\n",
        "      if np.isnan(value):\n",
        "        new_sample.append(rng.normal(loc, scale))\n",
        "      else:\n",
        "        value = np.log(value)\n",
        "        zscore = (value-smean)/sstd\n",
        "        value = (zscore*gstd)+gmean\n",
        "        value = np.exp(value)\n",
        "        new_sample.append(value)\n",
        "    new_values.append(new_sample)\n",
        "  #the .T is a transpose function which swaps rows for columns\n",
        "  return np.array(new_values).T\n"
      ],
      "metadata": {
        "id": "t_G6FHmBHp5h"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_proteins = proteins.copy()\n",
        "vec_proteins[abundance_cols] = vector_norm_impute(vec_proteins[abundance_cols].to_numpy())\n",
        "loop_proteins = proteins.copy()\n",
        "loop_proteins[abundance_cols] = loop_norm_impute(loop_proteins[abundance_cols].to_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwJ0saPEp_ip",
        "outputId": "86a4c208-6291-4e18-f0e3-1710b065e21d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running vector_norm_impute took 0.013887138999990611 seconds.\n",
            "Running loop_norm_impute took 0.45028108699997915 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3: Building and accessing dataframes\n"
      ],
      "metadata": {
        "id": "xhk_IqTyEHN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 6: protein summary statistics"
      ],
      "metadata": {
        "id": "Vq4hmcioEt4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the ideal approach for data processing if what you are doing can be\n",
        "#done with numpy. axis = 1 means that an operation is applied rowwise while\n",
        "#axis = 0 is applied columnwise\n",
        "@time_this\n",
        "def numpy_stats(df):\n",
        "  df['mean'] = np.nanmean(df[abundance_cols], axis = 1)\n",
        "  df['std'] = np.nanstd(df[abundance_cols], axis = 1)\n",
        "  return df\n",
        "\n",
        "#If the function can't be done with numpy (dictionary lookups are a common\n",
        "#example of this class of tasks) then this strategy is probably your best bet.\n",
        "#What we do here is zip together the columns we care about processing and then\n",
        "#apply a function over them within a list comprehension. Remember that zip\n",
        "#collects elements into tuples e.g. zip([1,2,3],[4,5,6]) -> ((1,4),(2,5),(3,6)).\n",
        "#We turn the zip into a list so that it can be used twice.\n",
        "@time_this\n",
        "def listcomp_stats(df):\n",
        "  abundance_iter = list(zip(*[df[c] for c in abundance_cols]))\n",
        "  df['mean'] = [np.nanmean(row) for row in abundance_iter]\n",
        "  df['std'] = [np.nanstd(row) for row in abundance_iter]\n",
        "  return df\n",
        "\n",
        "#Iterrows behaves similarly to the above code but is generally slower. It is\n",
        "#somewhat easier to read than the zip(*[df[c] for c in cols]) ideom above so for\n",
        "#uses where performance is not a concern it could be preferred.\n",
        "@time_this\n",
        "def iterrows_stats(df):\n",
        "  means = []\n",
        "  stds = []\n",
        "  #iterrows gives both the index and row at each iteration\n",
        "  for i,row in df[abundance_cols].iterrows():\n",
        "    #sequential calls to .append() are also slower than using list comprehension\n",
        "    means.append(np.nanmean(row))\n",
        "    stds.append(np.nanstd(row))\n",
        "  df['mean'] = means\n",
        "  df['std'] = stds\n",
        "  return df\n",
        "\n",
        "#Here we construct empty columns in the data frame and fill them element by\n",
        "#element using df.at to access individual values.\n",
        "#Accessing, and especially modifying, individual entries in a dataframe is\n",
        "#extremely slow and should be done only as a very last resort. If you are\n",
        "#filling or modifying an entire column or row then there is never a need to use\n",
        "#this approach.\n",
        "@time_this\n",
        "def at_stats(df):\n",
        "  #these two lines initialize empty columns\n",
        "  df['mean'] = ''\n",
        "  df['std'] = ''\n",
        "  for i,row in df[abundance_cols].iterrows():\n",
        "    df.at[i,'mean'] = np.nanmean(row)\n",
        "    df.at[i,'std'] = np.nanstd(row)\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "E16UWt2y2TI7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_df = numpy_stats(proteins.copy())\n",
        "listcomp_df = listcomp_stats(proteins.copy())\n",
        "iterrows_df = iterrows_stats(proteins.copy())\n",
        "at_df = at_stats(proteins.copy())"
      ],
      "metadata": {
        "id": "RqQlUiLb5GJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The pandas native way of doing this\n",
        "@time_this\n",
        "def melt(df):\n",
        "  df = df.melt(id_vars = key_cols, value_vars = abundance_cols)\n",
        "  return df\n",
        "\n",
        "#Here we make a list of dataframes, one per analyte column, that each have the\n",
        "#metadata columns and a single analyte intensity column. Finally this list is\n",
        "#concatenated together with a single call to pd.concat()\n",
        "@time_this\n",
        "def loop_melt(df):\n",
        "  dfs = [ ]\n",
        "  for a in abundance_cols:\n",
        "    tmp_df = df[[*key_cols,a]].copy()\n",
        "    #we have to rename the analyte column so that they all end up as a single\n",
        "    #column in the concatenated dataframe\n",
        "    tmp_df.columns = [*key_cols,'Abundnace']\n",
        "    #to keep track of what analyte each row represents we make a list of the\n",
        "    #column name with as many elements as the dataframe has rows and add that\n",
        "    #as a column\n",
        "    tmp_df['sample'] = [a]*df.shape[0]\n",
        "    dfs.append(tmp_df)\n",
        "  df = pd.concat(dfs)\n",
        "  return df\n",
        "\n",
        "#this works the same way as loop_melt() but at each iteration the temp_df is\n",
        "#concatenated to a growing dataframe. These repeated calls to pd.concat() are\n",
        "#expensive and result in noticably poorer performance.\n",
        "@time_this\n",
        "def accumulator_melt(df):\n",
        "  new_df = pd.DataFrame()\n",
        "  for a in abundance_cols:\n",
        "    tmp_df = df[[*key_cols,a]].copy()\n",
        "    tmp_df.columns = [*key_cols,'Abundnace']\n",
        "    tmp_df['sample'] = [a]*df.shape[0]\n",
        "    new_df = pd.concat([new_df, tmp_df])\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "xfRuVlONvWs0"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melt_df = melt(proteins.copy())\n",
        "print(melt_df.shape)\n",
        "loop_melt_df = loop_melt(proteins.copy())\n",
        "print(loop_melt_df.shape)\n",
        "accum_melt_df = accumulator_melt(proteins.copy())\n",
        "print(accum_melt_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLwdva2izOF4",
        "outputId": "bec048da-6e8e-4143-fee1-78b20ed6f75b"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running melt took 0.02813604199999986 seconds.\n",
            "(108306, 6)\n",
            "Running loop_melt took 0.04736300299998675 seconds.\n",
            "(108306, 6)\n",
            "Running accumulator_melt took 0.06905706599999917 seconds.\n",
            "(108306, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3: Black Box Optimizers"
      ],
      "metadata": {
        "id": "Hn6uV3phH4tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 7: isotopic packet fitting"
      ],
      "metadata": {
        "id": "2KGHUvp2Jpxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4: Multiprocessing"
      ],
      "metadata": {
        "id": "vcegauIsICTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1: Numba\n",
        "The standard implementation of python is interpreted. This means that when python code is executed a program called the interpreter goes through the code line-by-line and, by way of an intermediate representation, runs just that line. This has the advantage of flexability but it requires sigificant overhead and prevents the interpreter from analyzing and optimizing your code. This is in contrast with compiled languages where all of the code is read, analyzed, optimized, and turned into machine code that your CPU can understand before anything is run.\n",
        "\n",
        "Numba gives us a stepping stone between these two strategies. It is a just-in-time compiler for python funcitons. This allows us to take performance intensive funcitons and compile them into optimized machine code at runtime. For performance reasons not all python features are available within numba but if your function can be written with a combination of base python and numpy then it can proabably be optimized with numba.\n",
        "\n",
        "Another advantage of numba is that the compiled code is not subject to the GIL so calculations can be written to run in parallel.  "
      ],
      "metadata": {
        "id": "bL1vH6LTJyZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 8: custom correlation matrix"
      ],
      "metadata": {
        "id": "hSqT96OIJ2_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2: multiprocessing.Pool"
      ],
      "metadata": {
        "id": "nFxCOupHKMyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 9: bootstrapping\n"
      ],
      "metadata": {
        "id": "GTV8TwR6NLft"
      }
    }
  ]
}