{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOb/F9bJL7sx8e00+cSPPeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavis1/GST_colloquium/blob/main/GST_colloquium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GST Colloquium on Python coding\n"
      ],
      "metadata": {
        "id": "dvSqLtCbA3vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this allows us to interact with google drive from within python\n",
        "from google.colab import drive\n",
        "#we mount our google drive which means that we make it acessable as a hard drive\n",
        "#in our file system under the path '/content/drive'\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sC98RVF-BDsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#colab comes with most common packages installed already but we want to use a\n",
        "#few uncommon ones so we install these ourselves.\n",
        "#The \"!\" tells jupyter to pass the line to the bash shell, this allows us to run\n",
        "#command line tools like pip from within jupyter notebook.\n",
        "!pip install brain-isotopic-distribution\n",
        "!pip install multiprocesspandas\n",
        "!pip install xlsx2csv"
      ],
      "metadata": {
        "id": "gTQu0A6eB3qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it is generally recommended best practice to import the packages that come with\n",
        "#python first before importing third party packages.\n",
        "from collections import defaultdict\n",
        "from functools import cache\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from brainpy import isotopic_variants\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocesspandas\n",
        "from scipy.optimize import minimize\n",
        "from sortedcontainers import SortedList"
      ],
      "metadata": {
        "id": "_M3C91KvBy-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Structures\n",
        "\n",
        "The way data are stored in memory can have an enormous effect on the speed with which they can be processed for particular tasks. Choosing the correct data structure can be the difference between a runtime of hours or minutes in some situations."
      ],
      "metadata": {
        "id": "JJvq3pfCHxEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hashmaps\n",
        "\n",
        "Hashmaps are data structures that store key value pairs in a way that allows you to very quickly look up a value based on a key. Dictionaries in python are hashmaps which can store any object as a value and can use any hashable object as a key. Sets are specialized hashmaps which only store true or false as values. These can be used to efficiently look up whether or not a key is in the set.\n",
        "\n",
        "A hash is a function that maps objects of arbitrary size and complexity to a finite range of numbers. In the context of a hashmap data structure the finite range represents locations in memory and the hash function calculates where in memory a value is stored based on its key which means that the time it takes to find a value is only dependent on the time it takes to calculate the hash of a key. This contrasts with looking up values in a list which requires iterating over each item in the list until you find what you're looking for, an operation that becomes more expensive on average as the list grows in size.\n",
        "\n",
        "The tradeoff that you have to make with hashmaps is that there are some cases in which two keys have the same hash so their values must be stored in a list and iterated over for lookup. To minimize this problem the range of memory locations needs to be significantly larger than the number of values stored. This means that hash maps will generally be larger in memory than more compact data structures like lists. This is not as much of a problem as it first appears because Python does not store the actual value at the memory locaiton calculated by the hash function. Instead it stores another memory address which points to where the value is actually stored. This means that each block of memory indexed by the hashmap can be very small while still being able to point to values of arbitrary size. Therefore the memory overhead is only likely to be siginficant if you're storing a large number of small things."
      ],
      "metadata": {
        "id": "DBtrYU4mIrTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Search trees"
      ],
      "metadata": {
        "id": "1x8476GZIz9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 1: fuzzy matching"
      ],
      "metadata": {
        "id": "_yuGS-suI6Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 2: file parsing"
      ],
      "metadata": {
        "id": "wTzgj_HwJOyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 3: isotope abundance calculations"
      ],
      "metadata": {
        "id": "8xdqdzaoJan6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas and Numpy"
      ],
      "metadata": {
        "id": "fmc_HwHBHzip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Faster .xlsx import\n",
        "Large .xlsx files can be slow to import with pandas. This is because they are actually zipped folders of mostly XML files which need to be decompressed before going through the somewhat inefficient process of XML parsing. This setup gives excel the flexability to store images, graphs, formatting, and functions but, if all you want is a table of data, this flexability comes at significant performance costs. Thus it is generally preferable to store tabular data as a text file (.csv or .tsv) unless you absolutely need to store these   extra types of information.\n",
        "\n",
        "If you have no choice but to use large .xlsx files there is still a faster way to import the data as a pandas dataframe. The xlsx2csv package is sigificantly faster at parsing .xlsx files than pandas. So, what we can do is parse the .xlsx file to an in-memory .csv (meaning that the file is stored only in RAM and not written to our hard drive) then use the very efficient pandas .csv parser to get the data into a dataframe. This trick comes from [stackoverflow](https://stackoverflow.com/questions/28766133/faster-way-to-read-excel-files-to-pandas-dataframe#comment136215400_28769537)."
      ],
      "metadata": {
        "id": "C6KC1t-RN9To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 4: metabolomics data"
      ],
      "metadata": {
        "id": "DJVuSrn9Dq1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using vectorized operations"
      ],
      "metadata": {
        "id": "yjU_X23DDUpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 5: normalization\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PCnOKINlDvm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the comparison here will be iterrows vs my lz_norm approach"
      ],
      "metadata": {
        "id": "QGP0UvJnUORv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building and accessing dataframes"
      ],
      "metadata": {
        "id": "xhk_IqTyEHN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 6: protein summary statistics"
      ],
      "metadata": {
        "id": "Vq4hmcioEt4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df.at vs building a series of rows and using concatenate vs building a series of lists and using pd.DataFrame"
      ],
      "metadata": {
        "id": "6hATh0kIUbiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Black Box Optimizers"
      ],
      "metadata": {
        "id": "Hn6uV3phH4tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 7: isotopic packet fitting"
      ],
      "metadata": {
        "id": "2KGHUvp2Jpxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiprocessing"
      ],
      "metadata": {
        "id": "vcegauIsICTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Numba\n",
        "The standard implementation of python is interpreted. This means that when python code is executed a program called the interpreter goes through the code line-by-line and, by way of an intermediate representation, runs just that line. This has the advantage of flexability but it requires sigificant overhead and prevents the interpreter from analyzing and optimizing your code. This is in contrast with compiled languages where all of the code is read, analyzed, optimized, and turned into machine code that your CPU can understand before anything is run.\n",
        "\n",
        "Numba gives us a stepping stone between these two strategies. It is a just-in-time compiler for python funcitons. This allows us to take performance intensive funcitons and compile them into optimized machine code at runtime. For performance reasons not all python features are available within numba but if your function can be written with a combination of base python and numpy then it can proabably be optimized with numba.\n",
        "\n",
        "Another advantage of numba is that the compiled code is not subject to the GIL so calculations can be written to run in parallel.  "
      ],
      "metadata": {
        "id": "bL1vH6LTJyZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 8: custom correlation matrix"
      ],
      "metadata": {
        "id": "hSqT96OIJ2_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###multiprocessing.Pool"
      ],
      "metadata": {
        "id": "nFxCOupHKMyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 9: bootstrapping\n"
      ],
      "metadata": {
        "id": "GTV8TwR6NLft"
      }
    }
  ]
}