{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavis1/GST_colloquium/blob/main/GST_colloquium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GST Colloquium on Python coding\n",
        "\n",
        "\n",
        "*   random grab-bag of tips and tricks\n",
        "*   hyper-optimization is not encouraged in Python but these techniques can provide large performace gains for little-to-no extra work\n",
        "*   In many cases these optimizations don't matter much and we've chosen small examples for practicality but when working on large datasets they can provide major practical improvements\n",
        "*   examples are tasks our lab performs and negative examples are inspired by our own learning process\n",
        "\n"
      ],
      "metadata": {
        "id": "dvSqLtCbA3vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING ONLY\n",
        "#this allows us to interact with google drive from within python\n",
        "from google.colab import drive\n",
        "#we mount our google drive which means that we make it acessable as a hard drive\n",
        "#in our file system under the path '/content/drive'\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/GST_colloquium/* ."
      ],
      "metadata": {
        "id": "sC98RVF-BDsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#colab comes with most common packages installed already but we want to use a\n",
        "#few uncommon ones so we install these ourselves.\n",
        "#The \"!\" tells jupyter to pass the line to the bash shell, this allows us to run\n",
        "#command line tools like pip from within jupyter notebook.\n",
        "!pip install brain-isotopic-distribution\n",
        "!pip install multiprocesspandas\n",
        "!pip install xlsx2csv"
      ],
      "metadata": {
        "id": "gTQu0A6eB3qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it is generally recommended best practice to import the packages that come with\n",
        "#python first before importing third party packages.\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from functools import cache\n",
        "from multiprocessing import Pool\n",
        "from multiprocessing import Manager\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "from io import StringIO\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "from brainpy import isotopic_variants\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocesspandas import applyparallel\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import zscore\n",
        "from scipy.stats import binom\n",
        "from sortedcontainers import SortedList\n",
        "from xlsx2csv import Xlsx2csv\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "_M3C91KvBy-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This defines a function decorator, a concept which is used in ยง1.2.\n",
        "#It times a function call and prints the elapsed time while suppressing warnings\n",
        "def time_this(func):\n",
        "  #the way this works is that time_this() creates and returns a new function\n",
        "  #that replaces whatever funciton follows @time_this. But, because\n",
        "  def inner(*args, **kwargs):\n",
        "    #this context manager suppresses all warnings. DO NOT DO THIS NORMALLY\n",
        "    #We have extensively tested this code and data and know that the warnings\n",
        "    #can be safely ignored for this tutorial but that is not true for most code\n",
        "    with warnings.filterwarnings('ignore'):\n",
        "      #time.time() gives unix time in units of seconds\n",
        "      start_time = time.time()\n",
        "      #the try: ... except: block is a technique for gracefully handling errors.\n",
        "      #first the code under try: is run and if an exception is raised the code\n",
        "      #under except: is run.\n",
        "      try:\n",
        "        result = func(*args, **kwargs)\n",
        "      except:\n",
        "        #we want to still time a function call even if it fails so we catch any\n",
        "        #exception generated and use the traceback package to print the error\n",
        "        #message\n",
        "        traceback.print_exc()\n",
        "        #if func() fails it does not return anything so result is undefined\n",
        "        #in order to continue our script we need to define result\n",
        "        result = None\n",
        "      elapsed_time = time.time() - start_time\n",
        "    #prepending f to a string allows you to place expressions within braces {}\n",
        "    #which are evaluated and the result is placed within the string\n",
        "    print(f'Running {func.__name__} took {elapsed_time} seconds.')\n",
        "    return result\n",
        "  #we return the function inner which is then evaluated with the origional\n",
        "  #arguments. Using @time_this is equivalent to running\n",
        "  #time_this(func)(*args, **kwargs)\n",
        "  return inner\n",
        "\n",
        "#A test version without the warning suppression\n",
        "def time_this(func):\n",
        "  def inner(*args, **kwargs):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "      result = func(*args, **kwargs)\n",
        "    except:\n",
        "      traceback.print_exc()\n",
        "      result = None\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Running {func.__name__} took {elapsed_time} seconds.')\n",
        "    return result\n",
        "  return inner"
      ],
      "metadata": {
        "id": "1SOGhargJ13-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1: Data Structures\n",
        "\n",
        "The way data are stored in memory can have an enormous effect on the speed with which they can be processed for particular tasks. Choosing the correct data structure can be the difference between a runtime of hours or minutes in some situations."
      ],
      "metadata": {
        "id": "JJvq3pfCHxEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 Hashmaps\n",
        "\n",
        "Hashmaps are data structures that store key-value pairs in a way that allows you to very quickly look up a value (some data) based on a key (the datorum label). Dictionaries in python are hashmaps which can store any object as a value and can use any hashable object as a key. Sets are specialized hashmaps which only store true or false as values. These can be used to efficiently look up whether or not a key is in the set.\n",
        "\n",
        "A hash is a function that maps objects of arbitrary size and complexity to a finite range of numbers. In the context of a hashmap data structure the finite range represents locations in memory and the hash function calculates where in memory a value is stored based on its key which means that the time it takes to find a value is only dependent on the time it takes to calculate the hash of a key. This contrasts with looking up values in a list which requires iterating over each item in the list until you find what you're looking for, an operation that becomes more expensive on average as the list grows in size.\n",
        "\n",
        "\n",
        "<img align=\"right\" src=\"https://github.com/stavis1/GST_colloquium/blob/main/Hashmapfigure-margin.png?raw=true\" alt=\"Figure 1. Hashmap Data Call\" title=\"Figure 1: Accessing Data in a Hashmap\" width=\"440\" hspace=\"20\" vspace=\"10\"> The tradeoff that you have to make with hashmaps is that there are some cases in which two keys have the same hash so their values must be stored in a list and iterated over for lookup. To minimize this problem the range of memory locations needs to be significantly larger than the number of values stored. This means that hash maps will generally be larger in memory than more compact data structures like lists. This is not as much of a problem as it first appears because  Python does not store the actual value at the memory locaiton calculated by the hash function. Instead the memory location it calculates is the index of a list that stores memory addresses which point to where the values are actually stored (see figure). This means that each block of memory indexed by the hashmap can be very small while still being able to point to values of arbitrary size. Therefore the memory overhead is only likely to be siginficant if you're storing a large number of small things.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**TLDR:** Storing data in hashmaps (like a python dictionary) can significantly improve data access speed, thereby speeding up the whole script.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DBtrYU4mIrTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 1: file parsing\n"
      ],
      "metadata": {
        "id": "wTzgj_HwJOyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2: Memoization\n"
      ],
      "metadata": {
        "id": "eWRNHC19hRsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 2: isotope abundance calculations"
      ],
      "metadata": {
        "id": "8xdqdzaoJan6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3: Search trees"
      ],
      "metadata": {
        "id": "1x8476GZIz9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 3: fuzzy matching"
      ],
      "metadata": {
        "id": "_yuGS-suI6Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2: Pandas and Numpy"
      ],
      "metadata": {
        "id": "fmc_HwHBHzip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1: Faster .xlsx import\n",
        "Large .xlsx files can be slow to import with pandas. This is because they are actually zipped folders of mostly XML files which need to be decompressed before going through the somewhat inefficient process of XML parsing. This setup gives excel the flexability to store images, graphs, formatting, and functions but, if all you want is a table of data, this flexability comes at significant performance costs. Thus it is generally preferable to store tabular data as a text file (.csv or .tsv) unless you absolutely need to store these   extra types of information.\n",
        "\n",
        "If you have no choice but to use large .xlsx files there is still a faster way to import the data as a pandas dataframe. The xlsx2csv package is sigificantly faster at parsing .xlsx files than pandas. So, what we can do is parse the .xlsx file to an in-memory .csv (meaning that the file is stored only in RAM and not written to our hard drive) then use the very efficient pandas .csv parser to get the data into a dataframe. This trick comes from [stackoverflow](https://stackoverflow.com/questions/28766133/faster-way-to-read-excel-files-to-pandas-dataframe#comment136215400_28769537).\n"
      ],
      "metadata": {
        "id": "C6KC1t-RN9To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 4: metabolomics data\n",
        "metabolomics_data.xlsx is an example of the files produced during metabolomics experiments in our lab. It is an export of unfiltered search results from Compound Discoverer which we use for downstream analysis.\n",
        "\n",
        "For practicality purposes we have reduced the size of the .xlsx file for this demonstration significantly. In its original form (256547 rows by 506 columns) pandas .read_excel() took 16 minutes to load the file, the Xlsx2csv trick took 7 minutes, and reading in the .tsv version of the file with pandas .read_csv() took 16 seconds. The remarkable increase in speed between the original document and its current version is not merely due to cutting the number of rows in half. The extra formatting information that was included in the export, which was stripped during our file processing that went .xlsx -> .tsv -> .xlsx,  has a far larger impact. This can be seen in the relatively modest improvement in load time for the tsv reader as compared to the impressive improvements for the two direct .xlsx readers. We expect that these improvements are due to the removed formatting having massively complicated the XML representation of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "DJVuSrn9Dq1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The simple .tsv import\n",
        "@time_this\n",
        "def tsv_import(file_path):\n",
        "  df = pd.read_csv(file_path, sep = '\\t')\n",
        "  return df\n",
        "\n",
        "#The in-memory .csv conversion trick described above\n",
        "@time_this\n",
        "def fast_import(file_path):\n",
        "    #buffer is the in-memory variable in which the converted .csv is placed\n",
        "    buffer = StringIO()\n",
        "    #here we do the actual .xlsx parsing and conversion to .csv\n",
        "    #note that skip_hidden_rows is true by default unlike other .xlsx readers\n",
        "    Xlsx2csv(file_path,\n",
        "             outputencoding=\"utf-8\",\n",
        "             skip_hidden_rows = False).convert(buffer,sheetid=0)\n",
        "    buffer.seek(0)\n",
        "    #now we import the .csv with pandas exactly as if it were a regular file\n",
        "    df = pd.read_csv(buffer, low_memory=False, skiprows=1)\n",
        "    return df\n",
        "\n",
        "#the pandas native approach to excel import\n",
        "@time_this\n",
        "def pandas_import(file_path):\n",
        "  df = pd.read_excel(file_path, 0)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Lz-JyysMDvIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_df = tsv_import('metabolomics_data.tsv')\n",
        "fast_df = fast_import('metabolomics_data.xlsx')\n",
        "pandas_df = pandas_import('metabolomics_data.xlsx')"
      ],
      "metadata": {
        "id": "uIcKCg3PFDND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2: Using vectorized operations\n",
        "Numpy carries out most data processing operations using highly optimized pre-compiled code that is orders of magnitude faster than anything Python can natively accomplish. The optimizations include single instruction, multiple data (SIMD) steps in which a single operation that would normally be applied serially to values in an array can be applied simultaniously to blocks of 4-8 values. When doing mathematical operations it is therefore best to let numpy handle as much of the processing as possible. This means that we should avoid writing explicit loops or accessing individual values within numpy arrays as much as possible and instead perform operations on whole arrays at once using numpy functions.\n",
        "\n",
        "This strategy holds true for other packages like Pandas, Scipy, and Scikit-Learn as well because Numpy provides most of the data processing functionality for these packages.  "
      ],
      "metadata": {
        "id": "yjU_X23DDUpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 5: normalization\n",
        "Proteomic normalization and imputation are important to minimize non-biological variation and ensure reliability of your dataset. Normalization is used to minimize variation coming from factors such as sample preparation inconsistencies, injection volume errors, and fluctuations in instrument sensitivity, which can happen with extended use (i.e. sensitivity drift with instrument cleanliness, or column age). By accounting for these variations, normalization allows for the comparison of biological variances rather than technical ones. Following normalization, imputation fills missing values with low abundance numbers that mimic the instrument's noise floor, allowing for statistical testing across conditions. Both normalization and imputation create a baseline from which conditions can be compared in a more robust way.\n"
      ],
      "metadata": {
        "id": "PCnOKINlDvm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The data processing produces one file per condition that we read into a list\n",
        "#of dataframes\n",
        "prot_files = [f for f in os.listdir() if f.endswith('Proteins.txt')]\n",
        "dataframes = [pd.read_csv(f, sep = '\\t') for f in prot_files]\n",
        "key_cols = ['Accession','Description','MW [kDa]','# AAs']\n",
        "#We need to merge these data into a single dataframe for further processing\n",
        "proteins = dataframes.pop()\n",
        "for df in dataframes:\n",
        "  #on = key_cols is the list of columns that we use as a key for which rows to merge\n",
        "  #how = 'outer' means we use the union of these keys\n",
        "  proteins = proteins.merge(df, how = 'outer', on = key_cols)\n",
        "#we want a list of the columns containing protein abundances for processing\n",
        "abundance_cols = [c for c in proteins.columns if c.startswith('Abundance')]\n",
        "#For this example we only want to work with the key and abundance columns\n",
        "#so we get rid of all the other data in this dataframe.\n",
        "#The * operator unpacks an interable so [1,*[2,3,4]] becomes [1,2,3,4].\n",
        "proteins = proteins[[*key_cols, *abundance_cols]]"
      ],
      "metadata": {
        "id": "QGP0UvJnUORv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(1)\n",
        "\n",
        "@time_this\n",
        "def vector_norm_impute(values):\n",
        "  #processing is done in the log space because intensity is log-normally\n",
        "  #distributed across proteins\n",
        "  values = np.log(values)\n",
        "  #we calculated the mean and standard deviation of all values in all samples\n",
        "  gmean = np.nanmean(values)\n",
        "  gstd = np.nanstd(values)\n",
        "  #intensities are z-scored by column (axis = 0), which corresponds to files\n",
        "  values = zscore(values, axis = 0, nan_policy = 'omit')\n",
        "  #we un-do the z-scoring with the grand mean and standard deviation so that\n",
        "  #all files now have the same log-mean and log-standard deviation\n",
        "  values = (values*gstd)+gmean\n",
        "  #we impute by taking draws from a normal distribution centered 1.8 standard\n",
        "  #deviations below the mean to simulate the correlation between missingness and\n",
        "  #intensity\n",
        "  loc = gmean - (1.8*gstd)\n",
        "  scale = gstd*0.3\n",
        "  #The strategy here is to generate a random number for each value in our data\n",
        "  #and then only transfer random values to our data at the missing value\n",
        "  #positions. We do generate significantly more random values than necessary\n",
        "  #this way but the vectorized approach is so much faster than repeated calls to\n",
        "  #rng that it is still the preferred approach\n",
        "  imputed_values = rng.normal(loc, scale, values.shape)\n",
        "  mask = np.isnan(values) #an array of true/false for if a value is missing\n",
        "  values[mask] = imputed_values[mask] #only missing values are replaced\n",
        "  #the data are taken back out of log space for downstream processing\n",
        "  values = np.exp(values)\n",
        "  return values\n",
        "\n",
        "@time_this\n",
        "def loop_norm_impute(values):\n",
        "  #this is how you do a nested loop within a list comprehension\n",
        "  logvalues = [np.log(value) for sample in values for value in sample]\n",
        "  #as above we pre-calculate variables that depend on all values\n",
        "  gmean = np.nanmean(logvalues)\n",
        "  gstd = np.nanstd(logvalues)\n",
        "  loc = gmean - (1.8*gstd)\n",
        "  scale = gstd*0.3\n",
        "  #in this verison we construct a list of lists\n",
        "  new_values = []\n",
        "  for i in range(values.shape[1]):\n",
        "    sample = values[:,i]\n",
        "    new_sample = []\n",
        "    #in order to do the normalization elementwise we have to precalculate\n",
        "    #sample-specific values\n",
        "    smean = np.nanmean(sample)\n",
        "    sstd = np.nanstd(sample)\n",
        "    for value in sample:\n",
        "      #all the processing from here out is the same but done on one element\n",
        "      #at a time\n",
        "      if np.isnan(value):\n",
        "        new_sample.append(rng.normal(loc, scale))\n",
        "      else:\n",
        "        value = np.log(value)\n",
        "        zscore = (value-smean)/sstd\n",
        "        value = (zscore*gstd)+gmean\n",
        "        value = np.exp(value)\n",
        "        new_sample.append(value)\n",
        "    new_values.append(new_sample)\n",
        "  #the .T is a transpose function which swaps rows for columns\n",
        "  return np.array(new_values).T\n"
      ],
      "metadata": {
        "id": "t_G6FHmBHp5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_proteins = proteins.copy()\n",
        "vec_proteins[abundance_cols] = vector_norm_impute(vec_proteins[abundance_cols].to_numpy())\n",
        "loop_proteins = proteins.copy()\n",
        "loop_proteins[abundance_cols] = loop_norm_impute(loop_proteins[abundance_cols].to_numpy())"
      ],
      "metadata": {
        "id": "uwJ0saPEp_ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3: Building and accessing dataframes\n"
      ],
      "metadata": {
        "id": "xhk_IqTyEHN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 6: protein summary statistics"
      ],
      "metadata": {
        "id": "Vq4hmcioEt4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the ideal approach for data processing if what you are doing can be\n",
        "#done with numpy. axis = 1 means that an operation is applied rowwise while\n",
        "#axis = 0 is applied columnwise\n",
        "@time_this\n",
        "def numpy_stats(df):\n",
        "  df['mean'] = np.nanmean(df[abundance_cols], axis = 1)\n",
        "  df['std'] = np.nanstd(df[abundance_cols], axis = 1)\n",
        "  return df\n",
        "\n",
        "#If the function can't be done with numpy (dictionary lookups are a common\n",
        "#example of this class of tasks) then this strategy is probably your best bet.\n",
        "#What we do here is zip together the columns we care about processing and then\n",
        "#apply a function over them within a list comprehension. Remember that zip\n",
        "#collects elements into tuples e.g. zip([1,2,3],[4,5,6]) -> ((1,4),(2,5),(3,6)).\n",
        "#We turn the zip into a list so that it can be used twice.\n",
        "@time_this\n",
        "def listcomp_stats(df):\n",
        "  abundance_iter = list(zip(*[df[c] for c in abundance_cols]))\n",
        "  df['mean'] = [np.nanmean(row) for row in abundance_iter]\n",
        "  df['std'] = [np.nanstd(row) for row in abundance_iter]\n",
        "  return df\n",
        "\n",
        "#Iterrows behaves similarly to the above code but is generally slower. It is\n",
        "#somewhat easier to read than the zip(*[df[c] for c in cols]) ideom above so for\n",
        "#uses where performance is not a concern it could be preferred.\n",
        "@time_this\n",
        "def iterrows_stats(df):\n",
        "  means = []\n",
        "  stds = []\n",
        "  #iterrows gives both the index and row at each iteration\n",
        "  for i,row in df[abundance_cols].iterrows():\n",
        "    #sequential calls to .append() are also slower than using list comprehension\n",
        "    means.append(np.nanmean(row))\n",
        "    stds.append(np.nanstd(row))\n",
        "  df['mean'] = means\n",
        "  df['std'] = stds\n",
        "  return df\n",
        "\n",
        "#Here we construct empty columns in the data frame and fill them element by\n",
        "#element using df.at to access individual values.\n",
        "#Accessing, and especially modifying, individual entries in a dataframe is\n",
        "#extremely slow and should be done only as a very last resort. If you are\n",
        "#filling or modifying an entire column or row then there is never a need to use\n",
        "#this approach.\n",
        "@time_this\n",
        "def at_stats(df):\n",
        "  #these two lines initialize empty columns\n",
        "  df['mean'] = ''\n",
        "  df['std'] = ''\n",
        "  for i,row in df[abundance_cols].iterrows():\n",
        "    df.at[i,'mean'] = np.nanmean(row)\n",
        "    df.at[i,'std'] = np.nanstd(row)\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "E16UWt2y2TI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_df = numpy_stats(proteins.copy())\n",
        "listcomp_df = listcomp_stats(proteins.copy())\n",
        "iterrows_df = iterrows_stats(proteins.copy())\n",
        "at_df = at_stats(proteins.copy())"
      ],
      "metadata": {
        "id": "RqQlUiLb5GJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The pandas native way of doing this\n",
        "@time_this\n",
        "def melt(df):\n",
        "  df = df.melt(id_vars = key_cols, value_vars = abundance_cols)\n",
        "  return df\n",
        "\n",
        "#Here we make a list of dataframes, one per analyte column, that each have the\n",
        "#metadata columns and a single analyte intensity column. Finally this list is\n",
        "#concatenated together with a single call to pd.concat()\n",
        "@time_this\n",
        "def loop_melt(df):\n",
        "  dfs = [ ]\n",
        "  for a in abundance_cols:\n",
        "    tmp_df = df[[*key_cols,a]].copy()\n",
        "    #we have to rename the analyte column so that they all end up as a single\n",
        "    #column in the concatenated dataframe\n",
        "    tmp_df.columns = [*key_cols,'Abundnace']\n",
        "    #to keep track of what analyte each row represents we make a list of the\n",
        "    #column name with as many elements as the dataframe has rows and add that\n",
        "    #as a column\n",
        "    tmp_df['sample'] = [a]*df.shape[0]\n",
        "    dfs.append(tmp_df)\n",
        "  df = pd.concat(dfs)\n",
        "  return df\n",
        "\n",
        "#this works the same way as loop_melt() but at each iteration the temp_df is\n",
        "#concatenated to a growing dataframe. These repeated calls to pd.concat() are\n",
        "#expensive and result in noticably poorer performance.\n",
        "@time_this\n",
        "def accumulator_melt(df):\n",
        "  new_df = pd.DataFrame()\n",
        "  for a in abundance_cols:\n",
        "    tmp_df = df[[*key_cols,a]].copy()\n",
        "    tmp_df.columns = [*key_cols,'Abundnace']\n",
        "    tmp_df['sample'] = [a]*df.shape[0]\n",
        "    new_df = pd.concat([new_df, tmp_df])\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "xfRuVlONvWs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melt_df = melt(proteins.copy())\n",
        "print(melt_df.shape)\n",
        "loop_melt_df = loop_melt(proteins.copy())\n",
        "print(loop_melt_df.shape)\n",
        "accum_melt_df = accumulator_melt(proteins.copy())\n",
        "print(accum_melt_df.shape)\n"
      ],
      "metadata": {
        "id": "cLwdva2izOF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3: Black Box Optimizers"
      ],
      "metadata": {
        "id": "Hn6uV3phH4tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###scipy.optimize\n",
        "Optimizers are useful tools for a variety of tasks. If you ever find yourself doing a \"guess and check\" procedure, consider automating it with an optimizer function. The scipy package contains a variety of off-the-shelf black box optimizers that are easily applicable to a variety of situations. All you need is a way of numerically summarizing task \"success\". This is accomplished with a loss function (function to calculate error). The optimize will start with an initial guess you provide and iteratively adjust the value searching for an error minimum.\n",
        "\n",
        "**Possible Pitfall:** If your initial guess is too far off, the scipy optimizer can give pathological results, without nessesarily throwing an error."
      ],
      "metadata": {
        "id": "SVFyPjcDuP4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 7: isotopic packet fitting\n",
        "In one Hettich-lab project, it was nessesary to estimate the level of lipid deuteration by fitting a binomial distribution to isotope abundance data. Given the setup of the experiment, the binomial probability would correspond to the deuteration percent in the lipid. Someone whose name is definitely **not** Matthew built an excel spreadsheet to generate binomial distributions from a given probability. Matthew--\\*ahem\\*--this nameless person proceeded to generate binomial distributions from a variety of probabilities, plot them against the observed data, and generate new probabilites based on the perceived goodness of fit.\n",
        "\n",
        "This can be done more precisely and much faster using scipy.optimize.minimize. Basically the data are fed into minimize() which then changes the binomial probablility a bunch of times until it identifies a minimum point. In this case, we are calculating the root-mean squared error in our loss function."
      ],
      "metadata": {
        "id": "2KGHUvp2Jpxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class hashabledict(dict): #if needed to prevent collisions when memoizing formulae\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted(self.items())))\n",
        "\n",
        "@cache\n",
        "def formparser(chemformula): #This version uses memoize to speed up if there are many calculations with repeated formulae\n",
        "    chemform = chemformula.strip(\" \")\n",
        "    parsed = re.findall(r'([A-Z][a-z]?)(\\d*)',chemform)\n",
        "    composition = {p[0]:int(p[1]) if p[1] else 1 for p in parsed}\n",
        "    #return(hashabledict(composition))\n",
        "    return(composition)\n",
        "\n",
        "def deu_binom(totalhydrogen, pD, nonexH = 3): #Generate binomial distribution; need to adjust nonexH based on how many hydrogen in the formula are expected to back exchange with the solvent and be unlabeled\n",
        "    lit = []\n",
        "    for n in range(totalhydrogen-nonexH+1):\n",
        "        lit.append(binom.pmf(n, totalhydrogen-nonexH, pD))\n",
        "    arr = np.array(lit)\n",
        "    return arr\n",
        "\n",
        "@cache\n",
        "def natisodist(formuladict): # Generated natural isotope distribution for correction; uses memoize to potentially speed processing\n",
        "    theoretical_isotopic_cluster = isotopic_variants(formuladict, npeaks=5, charge=1)\n",
        "    array = np.empty(0)\n",
        "    for peak in theoretical_isotopic_cluster:\n",
        "        array = np.append(array, peak.intensity)\n",
        "    return(array)\n",
        "\n",
        "def natcor_deudist(formula, pD, scaled = True): #Combine natural isotope distribution with deuterium distibution\n",
        "    try:\n",
        "      pD =  pD[0] if hasattr(pD, '__len__') else pD\n",
        "      formdict = hashabledict(formparser(str(formula)))\n",
        "      nH = formdict.pop('H')\n",
        "      natdist = natisodist(formdict)\n",
        "      deudist = deu_binom(nH, pD)\n",
        "      totdist = np.convolve(natdist, deudist)\n",
        "      if scaled:\n",
        "          totdist = totdist/max(list(totdist))\n",
        "      return(totdist)\n",
        "    except:\n",
        "      print(formula, pD, scaled)\n",
        "      print(type(pD))\n",
        "\n",
        "def error(pD,formula,actualdata):\n",
        "    '''\n",
        "    calculate expected dist from elements given using above function.\n",
        "    calculate RMS error from actualdata as compared to calculated data.\n",
        "    '''\n",
        "    expecteddata = natcor_deudist(formula,pD)\n",
        "    actualdataext = np.append(actualdata, [0,0,0,0,0,0])\n",
        "    actualdatatrim = actualdataext[:len(expecteddata)]\n",
        "    return(mean_squared_error(actualdatatrim, expecteddata, squared=False))\n",
        "\n",
        "def optimize(pD, formula, actualdata):\n",
        "    '''\n",
        "    Use err function for least squared optimization of pD.\n",
        "    Takes RMS error calculated by err and varies pD to minimize RMSE\n",
        "    Returns least_squares optimization output\n",
        "    Needs pD to be in the ballpark of the best value, otherwise behaviour can be weird\n",
        "    Assumes Carbon, Hydrogen, Oxygen, Nitrogen, and actualdata are defined prior to using optimize\n",
        "    '''\n",
        "    results = minimize(error,\n",
        "                       x0 = [pD],\n",
        "                       bounds = [[0,1]],\n",
        "                       method = 'Nelder-Mead',\n",
        "                       args=(formula, actualdata)) #trf/dogbox\n",
        "    param = results.x\n",
        "    return(results)\n",
        "\n",
        "@time_this\n",
        "def deuterium_fitting(df):\n",
        "    int_cols = [*df.columns[7:]]\n",
        "    int_iter = list(zip(*[df[c] for c in int_cols]))\n",
        "    pDs = list(df[\"pD\"])\n",
        "    formula = list(df[\"Formula\"])\n",
        "    test = [optimize(pD, form, row) for row, pD, form in zip(int_iter, pDs, formula)]\n",
        "    maxiso = int(str(df.iloc[:,-1].name)[1:])\n",
        "    df[\"BestFit pD\"] = [t['x'][0] for t in test]\n",
        "    df[\"RMSE\"] = [t['fun'] for t in test]\n",
        "    df[\"fun termination\"] = [t['message'] for t in test]\n",
        "    def exp_dist(formula, x):\n",
        "        dist = natcor_deudist(formula, x)\n",
        "        dist = np.concatenate((dist,np.zeros(maxiso - len(dist))))\n",
        "        return dist\n",
        "    dists = np.array([exp_dist(f, t['x'][0]) for f,t in zip(formula, test)])\n",
        "    dists = pd.DataFrame(dists, columns= [f'isotope_{i}' for i in range(dists.shape[1])])\n",
        "    df = pd.concat([df,dists], axis = 1)\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "2lZEm2Q6q6Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"DL_Example_3-25-24.csv\", sep=',', encoding=\"UTF-8\")\n",
        "\n",
        "dfproc = deuterium_fitting(df)"
      ],
      "metadata": {
        "id": "8vLrEBg5R5Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4: Multiprocessing\n",
        "\n",
        "Most modern computers that we are likely to do any amount of bioinformatics on have more than one core, meaning they can process more than one instruction at a time. Python is not typically capable of taking advantage of this ability because it is constrained to run only one instruction at a time. This constriant has the advantage of making python code less prone to bugs and far easier to write, but sometimes we need the speed that parallelism provides. Python provides the multiprocessing package that provides a suite of tools for creating, running and communicating with multiple processes concurrently. A process is the in-memory representation of your code, associated data, and the instructions being run by the CPU.  \n",
        "\n",
        "There is a distinction to be made between logical and pysical CPU cores. A physical core is a self-contained computational unit capable of carrying out all the tasks we expect of a CPU. Having multiple physical cores always means that you can do multiple computations at once. Some physical cores are capable of doing two computations at a time if, and only if, those two tasks are different, e.g. adding two numbers while bit shifting another. This capacity is represented by the OS as \"logical cores\". If you're doing many different tasks having two logical cores is almost as good as having two physical cores but if there is significant overlap in tasks there's much less of an advantage. Google colab has only a single physical core with two logical cores. This is not a good processor for demonstrating the advantages of multiprocessing so be aware that you can expect far more dramatic performance improvements on other systems than what you see here.\n",
        "\n",
        "In the ideal case, splitting up a task across N cores will run in 1/N the time that the task took to run serially. However, this is never achieved in practice because the parallelization process itself requires extra work to carry out. In python specifically there is a lot of extra work to be done because the entire interpreter (the program that reads and runs your code) needs to be duplicated for each process.\n",
        "\n",
        "Additionally, in order to communicate between processes, e.g. to pass inputs to or results from a paralellized function, the data need to be serialized into a string of bytes then unpacked at the other end. This process can be qite slow for very large variables like dataframes or arrays. On Linux (and google colab runs on Linux) there is a workaround for this. The default way for a new process to be created on Linux is for the child process to share its parent memory until one of the processes modifies a chunk of that memory, at which point only that chunk is copied into a process-specific memory pool. This means that any variables available in the global scope when a child process is created remains available to that child process. On other operating systems this behavior is simulated by re-running the script from the top in each child process which can have significant performance costs if you aren't careful.\n",
        "\n"
      ],
      "metadata": {
        "id": "vcegauIsICTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1: multiprocessing.Pool\n",
        "Python's built-in multiprocessing package has many tools for paralell processing that allow an advanced user to set up complex archetectures. We, however, want something simple and the Pool module provides that. Pool will set up a pool of worker processes then, with a function and an iterable of inputs, will distribute the inputs across worker processes to be evaluated in parallel.\n",
        "We care about two methods that Pool provides: .map(func, jobs) works for functions with a single input, and .starmap(func, jobs) works for functions with multiple inputs where jobs = [(input1a, input1b, ...), ..., (inputNa, inputNb, ...)]"
      ],
      "metadata": {
        "id": "nFxCOupHKMyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function passes the input data to optimize() directly which means it must\n",
        "#be serialized to send it between processes\n",
        "@time_this\n",
        "def mp_row(lipid_data):\n",
        "  lipids = lipid_data.copy()\n",
        "  start_cols = list(lipids.columns)\n",
        "  input_cols = ['pD', 'Formula', *lipids.columns[7:]]\n",
        "  maxiso = int(str(lipids.iloc[:,-1].name)[1:])\n",
        "  jobs = zip(zip(*[lipids[c] for c in input_cols]), [maxiso]*lipids.shape[0])\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    with Pool() as p:\n",
        "      results = p.starmap(optimize, jobs)\n",
        "  results = pd.DataFrame(np.array(results)).T\n",
        "  lipids = pd.concat((lipids,results))\n",
        "  new_cols = [\"BestFit pD\",\n",
        "              \"RMSE\",\n",
        "              \"func termination\",\n",
        "              *[f'isotope_{i}' for i in range(results.shape[1] - 3)]]\n",
        "  lipids.columns = start_cols + new_cols\n",
        "  return lipids\n",
        "\n",
        "def idx_opt(i):\n",
        "  row = jobs[i]\n",
        "  return optimize(row,maxiso)\n",
        "\n",
        "@time_this\n",
        "def mp_idx(lipid_data):\n",
        "  lipids = lipid_data.copy()\n",
        "  start_cols = list(lipids.columns)\n",
        "  input_cols = ['pD', 'Formula', *lipids.columns[7:]]\n",
        "  global maxiso\n",
        "  maxiso = int(str(lipids.iloc[:,-1].name)[1:])\n",
        "  global jobs\n",
        "  jobs = list(zip(*[lipids[c] for c in input_cols]))\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "    with Pool() as p:\n",
        "      results = p.map(idx_opt, range(len(jobs)))\n",
        "  results = pd.DataFrame(np.array(results)).T\n",
        "  lipids = pd.concat((lipids,results))\n",
        "  new_cols = [\"BestFit pD\",\n",
        "              \"RMSE\",\n",
        "              \"func termination\",\n",
        "              *[f'isotope_{i}' for i in range(results.shape[1] - 3)]]\n",
        "  lipids.columns = start_cols + new_cols\n",
        "  return lipids"
      ],
      "metadata": {
        "id": "TTzyzdnypk49"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lipids = pd.read_csv('DL_Example_3-25-24.csv')\n",
        "\n",
        "row_results = mp_row(lipids)\n",
        "idx_results = mp_idx(lipids)"
      ],
      "metadata": {
        "id": "YBuaMGoSqo6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One downside of multiprocess.Pool is that if a fatal error is encountered in a function call only the process that encountered the error will termninate. This means that all the other jobs are run by the remaining worker processes before the error is communincated to the main process and your script finally stops. Debugging then becomes slow and painful. What follows is a code snippit that will skip function evaluation for all jobs if one worker encounters and error. This trick comes from [here.](https://superfastpython.com/multiprocessing-pool-stop-all-tasks/)"
      ],
      "metadata": {
        "id": "6A6B5Ann19zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Don't bother running this cell. It is only included to make it easy to\n",
        "#copy-paste it into your own code\n",
        "from multiprocessing import Pool\n",
        "from multiprocessing import Manager\n",
        "#this is a built-in package for printing out more informative error messages\n",
        "import traceback\n",
        "\n",
        "#the function we wish to parallelize\n",
        "def task(input):\n",
        "  #event is a shared object that can be seen by all child processes.\n",
        "  #if .is_set() is True that means there's been an error so function evaluation\n",
        "  #is skipped\n",
        "  if event.is_set():\n",
        "    return\n",
        "  #the try: ... except: block is a technique for gracefully handling errors.\n",
        "  #first the code under try: is run and if an exception is raised the code under\n",
        "  #except: is run.\n",
        "  try:\n",
        "    #your code here\n",
        "    return\n",
        "  except:\n",
        "    #we print the error message to the console\n",
        "    traceback.print_exc()\n",
        "    #and we set the event so that .is_set() returns True and all remaining\n",
        "    #function evaluations are skipped\n",
        "    event.set()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  jobs = [] #a list of inputs to task()\n",
        "  #the manager takes care of communicating the state of event between processes\n",
        "  with Manager() as manager:\n",
        "    shared_event = manager.Event()\n",
        "\n",
        "    #this function will be called first when starting a child process it makes\n",
        "    #event visible as a global variable within that process\n",
        "    def init_worker(shared_event):\n",
        "      global event\n",
        "      event = shared_event\n",
        "\n",
        "    with Pool(initializer=init_worker, initargs=(shared_event,)) as p:\n",
        "      results = p.map(task, jobs)"
      ],
      "metadata": {
        "id": "s3RI-O9R2lF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fail_optimize(i):\n",
        "  row = jobs[i]\n",
        "  pD, formula = row[:2]\n",
        "  actualdata = row[2:]\n",
        "  results = minimize(error,\n",
        "                      x0 = pD,\n",
        "                      bounds = [[0,1]],\n",
        "                      method = 'Nelder-Mead',\n",
        "                      args=(formula, actualdata))\n",
        "  param = results.x\n",
        "  dist = natcor_deudist(formula, param[0])\n",
        "  dist = np.concatenate((dist,np.zeros(maxiso - len(dist))))\n",
        "  results = (param[0], results.fun, results.message, *dist)\n",
        "  if i == 0:\n",
        "    raise Exception()\n",
        "  return results\n",
        "\n",
        "def safe_task(input):\n",
        "  if event.is_set():\n",
        "    return\n",
        "  try:\n",
        "    return fail_optimize(input)\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    event.set()\n",
        "    return None\n",
        "\n",
        "def unsafe_task(input):\n",
        "    return fail_optimize(input)\n",
        "\n",
        "@time_this\n",
        "def run_safe(lipid_data):\n",
        "  if __name__ == '__main__':\n",
        "    lipids = lipid_data.copy()\n",
        "    start_cols = list(lipids.columns)\n",
        "    input_cols = ['pD', 'Formula', *lipids.columns[7:]]\n",
        "    global maxiso\n",
        "    maxiso = int(str(lipids.iloc[:,-1].name)[1:])\n",
        "    global jobs\n",
        "    jobs = list(zip(*[lipids[c] for c in input_cols]))\n",
        "    with Manager() as manager:\n",
        "      shared_event = manager.Event()\n",
        "\n",
        "      def init_worker(shared_event):\n",
        "        global event\n",
        "        event = shared_event\n",
        "\n",
        "      with Pool(initializer=init_worker, initargs=(shared_event,)) as p:\n",
        "        return p.map(safe_task, range(len(jobs)))\n",
        "\n",
        "@time_this\n",
        "def run_unsafe(lipid_data):\n",
        "  if __name__ == '__main__':\n",
        "    lipids = lipid_data.copy()\n",
        "    start_cols = list(lipids.columns)\n",
        "    input_cols = ['pD', 'Formula', *lipids.columns[7:]]\n",
        "    global maxiso\n",
        "    maxiso = int(str(lipids.iloc[:,-1].name)[1:])\n",
        "    global jobs\n",
        "    jobs = list(zip(*[lipids[c] for c in input_cols]))\n",
        "    with Pool() as p:\n",
        "      return p.map(unsafe_task, range(len(jobs)))"
      ],
      "metadata": {
        "id": "tDUvnx0h5RYJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lipids = pd.read_csv('DL_Example_3-25-24.csv')\n",
        "run_safe(lipids)\n",
        "run_unsafe(lipids)"
      ],
      "metadata": {
        "id": "RgSMKFv7arB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 9: bootstrapping\n"
      ],
      "metadata": {
        "id": "GTV8TwR6NLft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2 Multiprocesspandas\n",
        "\n",
        "The multiprocesspandas package implements a drop-in replacement for the pandas .apply() called .apply_parallel() which allows you to easily apply a function to either rows or columns of a data frame in parallel. Parallelization reqires some amount of overhead so for simple functions that are fast to compute this overhead may result in the total runtime being longer than taking a serial approach. In these cases overhead can be minimized by trying different values for the n_chunks parameter, although the default should be ideal in most circumstances.\n",
        "\n",
        "Internally multiprocesspandas usess multiprocessing.pool so the considerations outlined in ยง4 apply here as well. Which, in light of colab only having 2 CPU cores is why we see very little improvement from parallelization in this situation."
      ],
      "metadata": {
        "id": "3hqZws3hlBLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(row, niso):\n",
        "    '''\n",
        "    Use err function for least squared optimization of pD.\n",
        "    Takes RMS error calculated by err and varies pD to minimize RMSE\n",
        "    Returns least_squares optimization output\n",
        "    Needs pD to be in the ballpark of the best value, otherwise behaviour can be weird\n",
        "    Assumes Carbon, Hydrogen, Oxygen, Nitrogen, and actualdata are defined prior to using optimize\n",
        "    '''\n",
        "    pD, formula = row[:2]\n",
        "    actualdata = row[2:]\n",
        "    results = minimize(error,\n",
        "                       x0 = pD,\n",
        "                       bounds = [[0,1]],\n",
        "                       method = 'Nelder-Mead',\n",
        "                       args=(formula, actualdata))\n",
        "    param = results.x\n",
        "    dist = natcor_deudist(formula, param[0])\n",
        "    dist = np.concatenate((dist,np.zeros(niso - len(dist))))\n",
        "    return (param[0], results.fun, results.message, *dist)\n",
        "\n",
        "@time_this\n",
        "def parallel_fit(lipid_data):\n",
        "    lipids = lipid_data.copy()\n",
        "    start_cols = list(lipids.columns)\n",
        "    input_cols = ['pD', 'Formula', *lipids.columns[7:]]\n",
        "    maxiso = int(str(lipids.iloc[:,-1].name)[1:])\n",
        "    results = lipids[input_cols].apply_parallel(optimize, niso = maxiso)\n",
        "    results = pd.DataFrame(np.array(results)).T\n",
        "    lipids = pd.concat((lipids,results))\n",
        "    new_cols = [\"BestFit pD\",\n",
        "                \"RMSE\",\n",
        "                \"func termination\",\n",
        "                *[f'isotope_{i}' for i in range(results.shape[1] - 3)]]\n",
        "    lipids.columns = start_cols + new_cols\n",
        "    return lipids\n",
        "\n",
        "\n",
        "@time_this\n",
        "def serial_fit(lipid_data):\n",
        "    lipids = lipid_data.copy()\n",
        "    start_cols = list(lipids.columns)\n",
        "    input_cols = ['pD', 'Formula', *lipids.columns[7:]]\n",
        "    maxiso = int(str(lipids.iloc[:,-1].name)[1:])\n",
        "    results = lipids[input_cols].apply(optimize, niso = maxiso, axis = 1)\n",
        "    results = pd.DataFrame(np.array(results)).T\n",
        "    lipids = pd.concat((lipids,results))\n",
        "    new_cols = [\"BestFit pD\",\n",
        "                \"RMSE\",\n",
        "                \"func termination\",\n",
        "                *[f'isotope_{i}' for i in range(results.shape[1] - 3)]]\n",
        "    lipids.columns = start_cols + new_cols\n",
        "    return lipids"
      ],
      "metadata": {
        "id": "xTs1oplKZRyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lipids = pd.read_csv('DL_Example_3-25-24.csv')\n",
        "\n",
        "parallel = parallel_fit(lipids)\n",
        "serial = serial_fit(lipids)"
      ],
      "metadata": {
        "id": "wgXMmklwnnCi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}